{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Data for the United States\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "## Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, date_add, col, split, year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld\n",
    "from pyspark.sql.types import StringType as Str, DoubleType as Dbl, IntegerType as Int, DateType as Date, LongType as Long\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this capstone project we are focusing on the immigration data for the US. The main idea is to build a broader view of the immigration behaviour to the US by joining it with a series of potentially interesting attributes such as:\n",
    "- the travel mode, seasonality, reason of travel, origin/destinations and avg. time spent in the US\n",
    "- the type of airports used to enter the US\n",
    "- the city demographics to which they're traveling\n",
    "- temperature statistics about the location where they are traveling from/to\n",
    "\n",
    "Given the above objective, the end solution will be a set of fact and dimensional tables hosted in Amazon S3 that the final user can easily access and analyse to generate new insights.\n",
    "\n",
    "An ETL data pipeline will be built to perform the following steps:  \n",
    "1. Extract data from different sources\n",
    "2. Process them with the help of Python and Spark\n",
    "3. Load the final tables into S3 in Parquet format (in this noteboook we will actually first load them locally).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The datasets we will be using are:\n",
    "- **I94 immigration data**: [this dataset](https://travel.trade.gov/research/reports/i94/historical/2016.html) comes from the US National Tourism and Trade Office and is available locally.\n",
    "- **Airport Code Table**: this is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "- **U.S. City Demographic data**: this dataset comes from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "- **World Temperature Data**: this dataset came from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "Below we will start by loading and exploring each dataset to have an idea of what steps are needed to run the complete data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since the complete immigration dataset ss very big, lets first **explore the sample dataset** provided in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 29)\n"
     ]
    }
   ],
   "source": [
    "# Read in the immigration sample dataset\n",
    "df_sample = pd.read_csv('immigration_data_sample.csv')\n",
    "print(df_sample.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "It has 1k rows and 29 columns: lets see which ones and compare with the full dataset above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 30) # extend default pandas max columns\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A data dictionary with definitions for each field is available in the provided file _I94_SAS_Labels_descriptions.SAS_ .  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For example, some of the main fields available are:  \n",
    "\n",
    "* i94yr: 4 digit year\n",
    "* i94mon: numeric month\n",
    "* i94cit/i94res: country code of the immigrant (respectively citizenship & residence)\n",
    "* i94port: arrival airport code\n",
    "* arrdate: arrival date in the USA in SAS numeric format\n",
    "* depdate: departure date from the USA in SA numeric format\n",
    "* i94mode: how the immigrant arrived (air/sea/land)\n",
    "* i94addr: US state of the immigrant? or of the arrival port? !!!!\n",
    "* i94visa: visa code collapsed into three categories (1=Business; 2 = Pleasure; 3=Student)\n",
    "* count: used for summary statistics\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Lets now try to **read the full dataset** with the help of **Spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "    load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "It's 3M rows dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_spark.show(3))\n",
    "print(df_spark.printSchema())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Full and sample dataset have the same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "##df_spark.write.parquet(\"sas_data\")\n",
    "##df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_spark.write.parquet(\"sas_data_mine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark2=spark.read.parquet(\"sas_data_mine\")\n",
    "df_spark2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.show(3)\n",
    "df_spark2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Cities Demographics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "print(df_demographics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset contains information about the **demographics of all US cities** and census-designated places with a population greater or equal to 65,000. More info [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/?dataChart=eyJxdWVyaWVzIjpbeyJjb25maWciOnsiZGF0YXNldCI6InVzLWNpdGllcy1kZW1vZ3JhcGhpY3MiLCJvcHRpb25zIjp7fX0sImNoYXJ0cyI6W3siYWxpZ25Nb250aCI6dHJ1ZSwidHlwZSI6ImNvbHVtbiIsImZ1bmMiOiJBVkciLCJ5QXhpcyI6Im1lZGlhbl9hZ2UiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjRkY1MTVBIn1dLCJ4QXhpcyI6ImNpdHkiLCJtYXhwb2ludHMiOjUwLCJzb3J0IjoiIn1dLCJ0aW1lc2NhbGUiOiIiLCJkaXNwbGF5TGVnZW5kIjp0cnVlLCJhbGlnbk1vbnRoIjp0cnVlfQ%3D%3D) .\n",
    "\n",
    "_Could be interesting to join this info with the immigration table above to better understand immigration flows and its impact on a city/state level. We should be able to join the two datasets via the US state code fields._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55075, 12)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_airports = pd.read_csv('airport-codes_csv.csv')\n",
    "print(df_airports.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This is a simple table of airport codes and corresponding cities. More info [here](https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "We can get more info about each airport (type, geographical coordinates, etc.). Again we should be able to join it with the immigration dataset via the US state code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### World temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    }
   ],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(fname)\n",
    "print(df_temperature.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Pretty big dataset with 8.5M rows and 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This dataset provides contains land temperatures by city. More info [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "_Might be interesting to analyse immigration flows by the temparature dimensions: e.g. are travelers with Tourism visa more likely to travel to a warmer place? What about business visa immigrants? We should be able to join this dataset with the immigration one via city/country fields._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Mapping codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Based on the immigration data dictionary file, I've also created and uploaded a couple of files providing the mappings for:  \n",
    "\n",
    "* country\n",
    "* arrival port\n",
    "* visa\n",
    "* mode of travel\n",
    "* state\n",
    "\n",
    "Lets read them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   code      country\n",
      "0   582      MEXICO \n",
      "1   236  AFGHANISTAN\n",
      "2   101      ALBANIA\n",
      "3   316      ALGERIA\n",
      "4   102      ANDORRA\n",
      "  code                  location state\n",
      "0  ALC                     ALCAN    AK\n",
      "1  ANC                 ANCHORAGE    AK\n",
      "2  BAR  BAKER AAF - BAKER ISLAND    AK\n",
      "3  DAC             DALTONS CACHE    AK\n",
      "4  PIZ    DEW STATION PT LAY DEW    AK\n",
      "   code          mode\n",
      "0     1           Air\n",
      "1     2           Sea\n",
      "2     3          Land\n",
      "3     4  Not Reported\n",
      "   code      visa\n",
      "0     1  Business\n",
      "1     2  Pleasure\n",
      "2     3   Student\n",
      "  code       state\n",
      "0   AL     ALABAMA\n",
      "1   AK      ALASKA\n",
      "2   AZ     ARIZONA\n",
      "3   AR    ARKANSAS\n",
      "4   CA  CALIFORNIA\n"
     ]
    }
   ],
   "source": [
    "map_country_codes = pd.read_csv('map_codes/map_country_codes.csv')\n",
    "print(map_country_codes.head())\n",
    "map_port_codes = pd.read_csv('map_codes/map_port_codes.csv')\n",
    "print(map_port_codes.head())\n",
    "\n",
    "map_mode_codes = pd.read_csv('map_codes/map_mode_codes.csv')\n",
    "print(map_mode_codes.head())\n",
    "map_visa_codes = pd.read_csv('map_codes/map_visa_codes.csv')\n",
    "print(map_visa_codes.head())\n",
    "map_state_codes = pd.read_csv('map_codes/map_state_codes.csv')\n",
    "print(map_state_codes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "print(df_demographics.shape)\n",
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "df_demographics.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Doesn't seem to have many missing values. Will not perform any cleaning for now.\n",
    "State Code and City are all populated, which is good as we might use thse fields to join with other tables to enrich the immigration dataset with statistics about US cities."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let see which is the aggregation level for this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearly is not just by City\n",
    "df_demographics['City'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2295, 12)\n",
      "(0, 12)\n"
     ]
    }
   ],
   "source": [
    "#Let see see if we remove duplicated rows by adding State and Race\n",
    "print(df_demographics[df_demographics[['City', 'State']].duplicated()].shape)\n",
    "print(df_demographics[df_demographics[['City', 'State', 'Race']].duplicated()].shape) # yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The table is aggegated by a combination of these 3 variables:  \n",
    "* City\n",
    "* State Code\n",
    "* Race"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Will take this into consideration for identifying distinct rows. No further cleaning is probably needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read it using Spark\n",
    "df_demographics = spark.read.csv('us-cities-demographics.csv', header = 'True', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "|       Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|             Asian| 4759|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.show(3)\n",
    "df_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- median_age: string (nullable = true)\n",
      " |-- male_population: string (nullable = true)\n",
      " |-- female_population: string (nullable = true)\n",
      " |-- number_veterans: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- avg_household_size: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics_clean = df_demographics.select(\n",
    "    col('City').alias('city'),\n",
    "    col('State').alias('state_name'),\n",
    "    col('Median Age').alias('median_age'),\n",
    "    col('Male Population').alias('male_population'),\n",
    "    col('Female Population').alias('female_population'),\n",
    "    col('Number of Veterans').alias('number_veterans'),\n",
    "    col('Foreign-born').alias('foreign_born'),\n",
    "    col('Average Household Size').alias('avg_household_size'),\n",
    "    col('State Code').alias('state_code'),\n",
    "    col('Race').alias('race'),\n",
    "    col('Count').alias('count'),\n",
    ")\n",
    "\n",
    "df_demographics_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55075, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_airports.shape)\n",
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "df_airports.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Which countries have most airports?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    22757\n",
       "BR     4334\n",
       "CA     2784\n",
       "AU     1963\n",
       "KR     1376\n",
       "Name: iso_country, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airports['iso_country'].value_counts().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Since eventually we are interested in performing the analysis only on immigrants arriving at US airports, we can keep just US data, so about 22k rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airports_us = df_airports.query(\"iso_country == 'US' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22757, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft      239\n",
       "continent       22756\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality      102\n",
       "gps_code         1773\n",
       "iata_code       20738\n",
       "local_code       1521\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_airports_us.shape)\n",
    "df_airports_us.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "I was hoping to use either iata_code or local_code as a unique identifier of the airport, but it seems there are too many missing values. Also I am not sure about the meaning of these two codes (data dictionary doesn't go in details), and when to use one or the other. Therefore I will use the field 'municipality' to join this dataset with others, which should correspond to the city of the airport.\n",
    "\n",
    "It has only 50 missing rows in the US subset: lets remove these records for our airports dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22655, 12)\n"
     ]
    }
   ],
   "source": [
    "df_airports_us = df_airports_us[df_airports_us['municipality'].notna()]\n",
    "print(df_airports_us.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Also we need to extract the following columns: state, longitude and latitude. \n",
    "\n",
    "Will do the rest of the cleaning with the help of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.csv('airport-codes_csv.csv', header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports.show(3)\n",
    "df_airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22757\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+------------------+--------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|state|          latitude|     longitude|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+------------------+--------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|   PA|-74.93360137939453|40.07080078125|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|   KS|       -101.473911|     38.704022|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|   AK|    -151.695999146|   59.94919968|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports_us_clean = df_airports.filter(\"iso_country == 'US'\")\\\n",
    "    .withColumn(\"state\", split(col(\"iso_region\"), \"-\")[1])\\\n",
    "    .withColumn(\"latitude\", split(col(\"coordinates\"), \",\")[0].cast(Dbl()))\\\n",
    "    .withColumn(\"longitude\", split(col(\"coordinates\"), \",\")[1].cast(Dbl()))\n",
    "\n",
    "print(df_airports_us_clean.count())\n",
    "df_airports_us_clean.show(3)\n",
    "df_airports_us_clean.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "At this point, we could also drop the following columns as not needed for the final tables: continent, iso_region, coordinates. Will do this in the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', 30) # extend default pandas max columns\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "cicid            0\n",
       "i94yr            0\n",
       "i94mon           0\n",
       "i94cit           0\n",
       "i94res           0\n",
       "i94port          0\n",
       "arrdate          0\n",
       "i94mode          0\n",
       "i94addr         59\n",
       "depdate         49\n",
       "i94bir           0\n",
       "i94visa          0\n",
       "count            0\n",
       "dtadfile         0\n",
       "visapost       618\n",
       "occup          996\n",
       "entdepa          0\n",
       "entdepd         46\n",
       "entdepu       1000\n",
       "matflag         46\n",
       "biryear          0\n",
       "dtaddto          0\n",
       "gender         141\n",
       "insnum         965\n",
       "airline         33\n",
       "admnum           0\n",
       "fltno            8\n",
       "visatype         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid|i94yr |i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto |gender|insnum|airline|admnum       |fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|6.0  |2016.0|4.0   |692.0 |692.0 |XXX    |20573.0|null   |null   |null   |37.0  |2.0    |1.0  |null    |null    |null |T      |null   |U      |null   |1979.0 |10282016|null  |null  |null   |1.897628485E9|null |B2      |\n",
      "|7.0  |2016.0|4.0   |254.0 |276.0 |ATL    |20551.0|1.0    |AL     |null   |25.0  |3.0    |1.0  |20130811|SEO     |null |G      |null   |Y      |null   |1991.0 |D/S     |M     |null  |null   |3.73679633E9 |00296|F1      |\n",
      "|15.0 |2016.0|4.0   |101.0 |101.0 |WAS    |20545.0|1.0    |MI     |20691.0|55.0  |2.0    |1.0  |20160401|null    |null |T      |O      |null   |M      |1961.0 |09302016|M     |null  |OS     |6.66643185E8 |93   |B2      |\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_spark.show(3, truncate = False))\n",
    "print(df_spark.printSchema())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Lets test all the cleaning steps on the sample datasets, using Spark (since eventually we''l have to use Spark for the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|2027561|4084316.0|2016.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|56582674633.0|00782|      WT|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_spark_sample = spark.read.csv('immigration_data_sample.csv', header = 'True')\n",
    "df_spark_sample.show(1)\n",
    "print(df_spark_sample.printSchema())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Ok, lets work with it first..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "First thing we might want to do, is to add mapping codes from mapping tables and remove unnecessary fields. \n",
    "Also we  need to convert arrival/departure date fields (which are in SAS format) into proper date fields. SAS format should indicate the number of days since 1960-01-01.\n",
    "\n",
    "Let's create some temporary tables to work in SparkSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|code|mode|\n",
      "+----+----+\n",
      "|   1| Air|\n",
      "|   2| Sea|\n",
      "|   3|Land|\n",
      "+----+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+--------+\n",
      "|code|    visa|\n",
      "+----+--------+\n",
      "|   1|Business|\n",
      "|   2|Pleasure|\n",
      "|   3| Student|\n",
      "+----+--------+\n",
      "\n",
      "+----+-------+\n",
      "|code|  state|\n",
      "+----+-------+\n",
      "|  AL|ALABAMA|\n",
      "|  AK| ALASKA|\n",
      "|  AZ|ARIZONA|\n",
      "+----+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|code|            location|state|\n",
      "+----+--------------------+-----+\n",
      "| ALC|               ALCAN|   AK|\n",
      "| ANC|           ANCHORAGE|   AK|\n",
      "| BAR|BAKER AAF - BAKER...|   AK|\n",
      "+----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 582|    MEXICO |\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "+----+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read in mapping files with Spark\n",
    "map_mode_codes = spark.read.csv('map_codes/map_mode_codes.csv', header = 'True')\n",
    "map_mode_codes.show(3)\n",
    "map_visa_codes = spark.read.csv('map_codes/map_visa_codes.csv', header = 'True')\n",
    "map_visa_codes.show(3)\n",
    "map_state_codes = spark.read.csv('map_codes/map_state_codes.csv', header = 'True')\n",
    "map_state_codes.show(3)\n",
    "\n",
    "map_port_codes = spark.read.csv('map_codes/map_port_codes.csv', header = 'True')\n",
    "map_port_codes.show(3)\n",
    "map_country_codes = spark.read.csv('map_codes/map_country_codes.csv', header = 'True')\n",
    "map_country_codes.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_spark_sample.createOrReplaceTempView(\"df_immigration\")  # to use sample dataset\n",
    "df_spark.createOrReplaceTempView(\"df_immigration\")  # to use full dataset\n",
    "map_mode_codes.createOrReplaceTempView(\"map_mode_codes\")\n",
    "map_visa_codes.createOrReplaceTempView(\"map_visa_codes\")\n",
    "map_state_codes.createOrReplaceTempView(\"map_state_codes\")\n",
    "\n",
    "map_port_codes.createOrReplaceTempView(\"map_port_codes\")\n",
    "map_country_codes.createOrReplaceTempView(\"map_country_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "\n",
      "+----+----+\n",
      "|code|mode|\n",
      "+----+----+\n",
      "|   1| Air|\n",
      "|   2| Sea|\n",
      "|   3|Land|\n",
      "+----+----+\n",
      "\n",
      "+----+--------+\n",
      "|code|    visa|\n",
      "+----+--------+\n",
      "|   1|Business|\n",
      "|   2|Pleasure|\n",
      "|   3| Student|\n",
      "+----+--------+\n",
      "\n",
      "+----+-------+\n",
      "|code|  state|\n",
      "+----+-------+\n",
      "|  AL|ALABAMA|\n",
      "|  AK| ALASKA|\n",
      "|  AZ|ARIZONA|\n",
      "+----+-------+\n",
      "\n",
      "+----+--------------------+-----+\n",
      "|code|            location|state|\n",
      "+----+--------------------+-----+\n",
      "| ALC|               ALCAN|   AK|\n",
      "| ANC|           ANCHORAGE|   AK|\n",
      "| BAR|BAKER AAF - BAKER...|   AK|\n",
      "+----+--------------------+-----+\n",
      "\n",
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 582|    MEXICO |\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Try to query via sql\n",
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM df_immigration\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM df_immigration\n",
    "limit 3\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM map_mode_codes\n",
    "limit 3\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM map_visa_codes\n",
    "limit 3\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM map_state_codes\n",
    "limit 3\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM map_port_codes\n",
    "limit 3\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM map_country_codes\n",
    "limit 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_joined = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        i.i94yr as year,\n",
    "        i.i94mon as month,\n",
    "        i.i94cit as citizenship_country,\n",
    "        i.i94res as residence_country,\n",
    "        i.i94port as port,\n",
    "        i.arrdate,\n",
    "        date_add(to_date('1960-01-01'), i.arrdate) AS arrival_date,\n",
    "        coalesce(m.mode, 'Not reported') as arrival_mode,\n",
    "        coalesce(s.code, 'Not reported') as us_state,\n",
    "        i.depdate,\n",
    "        date_add(to_date('1960-01-01'), i.depdate) AS departure_date,\n",
    "        i.i94bir as respondent_age,\n",
    "        coalesce(v.visa, 'Not reported') as visa,\n",
    "        i.dtadfile as date_added,\n",
    "        i.visapost as visa_issued_department,\n",
    "        i.occup as occupation,\n",
    "        i.entdepa as arrival_flag,\n",
    "        i.entdepd as departure_flag,\n",
    "        i.entdepu as update_flag,\n",
    "        i.matflag as match_arrival_departure_fag,\n",
    "        i.biryear as birth_year,\n",
    "        i.dtaddto as allowed_to_stay_date,\n",
    "        i.insnum as ins_number,\n",
    "        i.airline as airline,\n",
    "        i.admnum as admission_number,\n",
    "        i.fltno as flight_number,\n",
    "        i.visatype as visa_type\n",
    "    from df_immigration i \n",
    "    left join map_mode_codes m on i.i94mode = m.code\n",
    "    left join map_visa_codes v on i.i94visa = v.code\n",
    "    left join map_state_codes s on i.i94addr = s.code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+-----------------+----+-------+------------+------------+------------+-------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+\n",
      "|  year|month|citizenship_country|residence_country|port|arrdate|arrival_date|arrival_mode|    us_state|depdate|departure_date|respondent_age|    visa|date_added|visa_issued_department|occupation|arrival_flag|departure_flag|update_flag|match_arrival_departure_fag|birth_year|allowed_to_stay_date|ins_number|airline|admission_number|flight_number|visa_type|\n",
      "+------+-----+-------------------+-----------------+----+-------+------------+------------+------------+-------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+\n",
      "|2016.0|  4.0|              692.0|            692.0| XXX|20573.0|  2016-04-29|Not reported|Not reported|   null|          null|          37.0|Pleasure|      null|                  null|      null|           T|          null|          U|                       null|    1979.0|            10282016|      null|   null|   1.897628485E9|         null|       B2|\n",
      "|2016.0|  4.0|              254.0|            276.0| ATL|20551.0|  2016-04-07|         Air|          AL|   null|          null|          25.0| Student|  20130811|                   SEO|      null|           G|          null|          Y|                       null|    1991.0|                 D/S|      null|   null|    3.73679633E9|        00296|       F1|\n",
      "|2016.0|  4.0|              101.0|            101.0| WAS|20545.0|  2016-04-01|         Air|          MI|20691.0|    2016-08-25|          55.0|Pleasure|  20160401|                  null|      null|           T|             O|       null|                          M|    1961.0|            09302016|      null|     OS|    6.66643185E8|           93|       B2|\n",
      "+------+-----+-------------------+-----------------+----+-------+------------+------------+------------+-------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- citizenship_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_mode: string (nullable = false)\n",
      " |-- us_state: string (nullable = false)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- respondent_age: double (nullable = true)\n",
      " |-- visa: string (nullable = false)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- visa_issued_department: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- arrival_flag: string (nullable = true)\n",
      " |-- departure_flag: string (nullable = true)\n",
      " |-- update_flag: string (nullable = true)\n",
      " |-- match_arrival_departure_fag: string (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- allowed_to_stay_date: string (nullable = true)\n",
      " |-- ins_number: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admission_number: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_joined.show(3)\n",
    "df_immigration_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration_joined.createOrReplaceTempView(\"df_immigration_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+------------+-------+--------------+--------------+----------+\n",
      "|  year|month|arrdate|arrival_date|depdate|departure_date|respondent_age|birth_year|\n",
      "+------+-----+-------+------------+-------+--------------+--------------+----------+\n",
      "|2016.0|  4.0|20573.0|  2016-04-29|   null|          null|          37.0|    1979.0|\n",
      "|2016.0|  4.0|20551.0|  2016-04-07|   null|          null|          25.0|    1991.0|\n",
      "|2016.0|  4.0|20545.0|  2016-04-01|20691.0|    2016-08-25|          55.0|    1961.0|\n",
      "|2016.0|  4.0|20545.0|  2016-04-01|20567.0|    2016-04-23|          28.0|    1988.0|\n",
      "|2016.0|  4.0|20545.0|  2016-04-01|20567.0|    2016-04-23|           4.0|    2012.0|\n",
      "+------+-----+-------+------------+-------+--------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look into some numerical variables, and in particular the calculated dates\n",
    "spark.sql(\"\"\"\n",
    "SELECT year, month, arrdate, arrival_date, depdate, departure_date, respondent_age, birth_year\n",
    "FROM df_immigration_joined\n",
    "limit 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Dates look fine now. However we can check if there is any case with arrival_date > departure_date. In this case it wouldn't be correct, and we might decide to remove them. Remember that arrival_date is the date of entry in the US, while departure_date is when the person left the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     375|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM df_immigration_joined\n",
    "    WHERE arrival_date > departure_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  142457|\n",
      "+--------+\n",
      "\n",
      "+--------------------------------+\n",
      "|count(DISTINCT admission_number)|\n",
      "+--------------------------------+\n",
      "|                         3075579|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------+\n",
      "|num_char|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n",
      "+-----------------+-----------------+-------------------+-------------------+\n",
      "|min(arrival_date)|max(arrival_date)|min(departure_date)|max(departure_date)|\n",
      "+-----------------+-----------------+-------------------+-------------------+\n",
      "|       2016-04-01|       2016-04-30|         2001-07-20|         2084-05-16|\n",
      "+-----------------+-----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check other fields\n",
    "\n",
    "# how many recors have either arrival or departure date NULL: 4% of total records\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM df_immigration_joined\n",
    "    WHERE arrival_date is null\n",
    "    or departure_date is null\n",
    "\"\"\").show()\n",
    "\n",
    "# admission number might be a unique identifier for the table\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT (DISTINCT admission_number)\n",
    "FROM df_immigration_joined\n",
    "\"\"\").show()\n",
    "\n",
    "# check quality of port field: should be 3 characters long\n",
    "spark.sql(\"\"\"\n",
    "SELECT LENGTH(port) as num_char\n",
    "FROM df_immigration_joined\n",
    "GROUP BY num_char\n",
    "\"\"\").show()\n",
    "\n",
    "# Only data from April 2016 as expected\n",
    "spark.sql(\"\"\"\n",
    "SELECT min(arrival_date), max(arrival_date), min(departure_date), max(departure_date)\n",
    "FROM df_immigration_joined\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|citizenship_country|     n|\n",
      "+-------------------+------+\n",
      "|              135.0|360157|\n",
      "|              209.0|206873|\n",
      "|              245.0|191425|\n",
      "|              111.0|188766|\n",
      "|              582.0|175781|\n",
      "|              148.0|157806|\n",
      "|              254.0|137735|\n",
      "|              689.0|129833|\n",
      "|              213.0|110691|\n",
      "|              438.0|109884|\n",
      "|              117.0| 78535|\n",
      "|              123.0| 76920|\n",
      "|              687.0| 69853|\n",
      "|              129.0| 57224|\n",
      "|              691.0| 54120|\n",
      "|              130.0| 45269|\n",
      "|              251.0| 41744|\n",
      "|              692.0| 41349|\n",
      "|              252.0| 41132|\n",
      "|              696.0| 40785|\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+------+\n",
      "|residence_country|     n|\n",
      "+-----------------+------+\n",
      "|            135.0|368421|\n",
      "|            209.0|249167|\n",
      "|            245.0|185609|\n",
      "|            111.0|185339|\n",
      "|            582.0|179603|\n",
      "|            112.0|156613|\n",
      "|            276.0|136312|\n",
      "|            689.0|134907|\n",
      "|            438.0|112407|\n",
      "|            213.0|107193|\n",
      "|            687.0| 75128|\n",
      "|            123.0| 74619|\n",
      "|            117.0| 65782|\n",
      "|            691.0| 54330|\n",
      "|            129.0| 49138|\n",
      "|            131.0| 47969|\n",
      "|            130.0| 45063|\n",
      "|            251.0| 43435|\n",
      "|            692.0| 42495|\n",
      "|            696.0| 40979|\n",
      "+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets maybe provide codes for these too, as we did for state\n",
    "spark.sql(\"\"\"\n",
    "SELECT citizenship_country, count(*) as n\n",
    "FROM df_immigration_joined\n",
    "GROUP BY citizenship_country\n",
    "order by n desc\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT residence_country, count(*) as n\n",
    "FROM df_immigration_joined\n",
    "GROUP BY residence_country\n",
    "order by n desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    respondent_age|\n",
      "+-------+------------------+\n",
      "|  count|           3095511|\n",
      "|   mean|41.767614458485205|\n",
      "| stddev| 17.42026053458826|\n",
      "|    min|              -3.0|\n",
      "|    max|             114.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we could filter out kids? But maybe parents filled out the immigration questionnaire on their behalf, so do not do anything on these. Just remove if there happen to be any negative age.\n",
    "df_immigration_joined.describe(['respondent_age']).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Additional cleaning: dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2953435"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_cleaned = spark.sql(\"\"\"\n",
    "    select *\n",
    "    from df_immigration_joined\n",
    "    where 1=1\n",
    "        and respondent_age >= 0\n",
    "        and arrival_date IS NOT NULL \n",
    "        and departure_date IS NOT NULL\n",
    "        and arrival_date <= departure_date\n",
    "\"\"\")\n",
    "\n",
    "df_immigration_cleaned.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "After cleaning, we're left with over 95% of total records."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create final fact table with immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- citizenship_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_mode: string (nullable = false)\n",
      " |-- us_state: string (nullable = false)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- respondent_age: double (nullable = true)\n",
      " |-- visa: string (nullable = false)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- visa_issued_department: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- arrival_flag: string (nullable = true)\n",
      " |-- departure_flag: string (nullable = true)\n",
      " |-- update_flag: string (nullable = true)\n",
      " |-- match_arrival_departure_fag: string (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- allowed_to_stay_date: string (nullable = true)\n",
      " |-- ins_number: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admission_number: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = df_immigration_cleaned.\\\n",
    "    drop('arrdate', 'depdate')\n",
    "fact_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Generate a dimension time table using the Immigration fact table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "To do this we will extract distinct dates from arrival/departure date fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_time = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT arrival_date AS date\n",
    "    FROM fact_immigration\n",
    "    WHERE arrival_date IS NOT NULL\n",
    "    UNION\n",
    "    SELECT DISTINCT departure_date AS date\n",
    "    FROM fact_immigration\n",
    "    WHERE departure_date IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2016-04-25|\n",
      "|2016-05-03|\n",
      "|2016-08-31|\n",
      "|2016-07-26|\n",
      "|2016-08-15|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_time.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+\n",
      "|      date|day|week|month|year|\n",
      "+----------+---+----+-----+----+\n",
      "|2016-04-25| 25|  17|    4|2016|\n",
      "|2016-05-03|  3|  18|    5|2016|\n",
      "|2016-08-31| 31|  35|    8|2016|\n",
      "|2016-07-26| 26|  30|    7|2016|\n",
      "|2016-08-15| 15|  33|    8|2016|\n",
      "+----------+---+----+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_time = dim_time.select(\n",
    "    col('date').alias('date'),\n",
    "    dayofmonth('date').alias('day'),\n",
    "    weekofyear('date').alias('week'),\n",
    "    month('date').alias('month'),\n",
    "    year('date').alias('year')\n",
    ")\n",
    "\n",
    "dim_time.show(5)\n",
    "dim_time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8599212, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_temperature.shape)\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_temperature.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1743-11-01\n",
      "2013-09-01\n"
     ]
    }
   ],
   "source": [
    "print(df_temperature['dt'].min())\n",
    "print(df_temperature['dt'].max())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "We don't actually have any statistics for 2016, year on which we are working on in the fact table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "df_temperature.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Which countries have more observations?\n",
    "df_temperature['Country'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "What is the aggregation level of this dataset? Date + City + Country is probably not enough. Let's understand this, before moving on with cleaning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46034, 7)\n",
      "(0, 7)\n"
     ]
    }
   ],
   "source": [
    "#Let see see if we remove duplicated rows by adding variables\n",
    "print(df_temperature[df_temperature[['dt', 'City', 'Country']].duplicated()].shape)\n",
    "print(df_temperature[df_temperature[['dt', 'City', 'Country', 'Latitude', 'Longitude']].duplicated()].shape) #yes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "Observations are recorded by: Date + City + Country + latitude + longitude."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "But in the end, longitude and latitude will not be very useful if we perform an analysis at a City level. \n",
    "\n",
    "Therefore our cleaning could include the following steps:\n",
    "- remove rows with missing 'AverageTemperature' (we could have imputed missing values with closest ones in terms of date/city or even lat/long, but in this instance I think could be risky as it would much deeper investigation to understand for example if there are big period gaps in terms of days for a specific City, which could result in a very imprecise assignation)\n",
    "- remove latitude/longitude fields\n",
    "- remove dates prior to 1960 (when mass air travel began https://www.insider.com/air-travel-in-every-decade-2017-8). The alternative is to keep only dates greater than min(date) in dim_time table, to make sure to cover all dates included in fact table (I commented this condition below, but not included for now as our fact table has only 2016 data).\n",
    "- compute avg. statistics by day/city/country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read it into Spark\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.csv(fname, header = 'True')\n",
    "\n",
    "df_temperature.createOrReplaceTempView(\"df_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8599212\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_temperature.count())\n",
    "df_temperature.show(3)\n",
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#testing SparkSQL error\n",
    "df_temperature.createOrReplaceTempView(\"df_temperature\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "*\n",
    "FROM df_temperature\n",
    "limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_world_temperatures = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dt as date,\n",
    "    City as city,\n",
    "    Country as country,\n",
    "    avg(AverageTemperature) as avg_temperature,\n",
    "    avg(AverageTemperatureUncertainty) as avg_temperature_uncertainty\n",
    "    --count(*)\n",
    "FROM df_temperature\n",
    "WHERE 1=1\n",
    "    and dt >='1960-01-01'\n",
    "    --and dt >= (select min(date) from dim_time) --make sure we cover all dates we have in the fact table\n",
    "    and AverageTemperature is not null\n",
    "group by date, city, country\n",
    "--limit 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247991\n",
      "+----------+-----+-------+-------------------+---------------------------+\n",
      "|      date| city|country|    avg_temperature|avg_temperature_uncertainty|\n",
      "+----------+-----+-------+-------------------+---------------------------+\n",
      "|1976-01-01|Århus|Denmark|0.14699999999999994|                       0.08|\n",
      "|1969-11-01|Çorlu| Turkey|             12.213|                      0.225|\n",
      "|1981-05-01|Çorlu| Turkey| 14.369000000000002|                       0.21|\n",
      "|1992-07-01|Çorlu| Turkey| 21.534000000000002|        0.28800000000000003|\n",
      "|1969-08-01|Çorum| Turkey| 20.980999999999998|                      0.308|\n",
      "+----------+-----+-------+-------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- avg_temperature: double (nullable = true)\n",
      " |-- avg_temperature_uncertainty: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dim_world_temperatures.count())\n",
    "dim_world_temperatures.show(5)\n",
    "dim_world_temperatures.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "The model chosen is a star schema with immigration data at the center of our schema (fact table), and a series of dimensional tables that provides additional information about dimensions like:\n",
    "- airports\n",
    "- US cities demographics\n",
    "- world temperatures\n",
    "\n",
    "A time dimensional table is also available for dates conversion convenience. And two extra mapping tables for port codes and country codes are provided to perfom join across all tables. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "The final schema will look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![data_model](img/data_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "The Data Pipeline perform the following steps:\n",
    "1. Extract raw data from a series of different sources (see below)\n",
    "2. Process it using Spark\n",
    "3. Load the data into a set of fact/dimensional tables in Parquet format\n",
    "\n",
    "Data is extracted from the folowing sources:\n",
    "- SAS files: data about immigration, which will eventually be loaded into a fact table\n",
    "- .csv files: data about airports, cities, temperatures which will be loaded into a series of dimension tables. In addition to these, we also use a series of .csv mapping files loaded in a local folder.\n",
    "\n",
    "Below we report the main cleaning steps performed to create each of the fact/dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Fact Immigration**  \n",
    "* add mapping codes for state, visa, travel mode (port and country codes are kept in separate tables)\n",
    "* convert SAS date fields from numeric to datetime format\n",
    "* remove unnecessary fields: count, original SAS date fields and possibly others\n",
    "* create final fact table\n",
    "* write immigration fact table to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Time Dimension**  \n",
    "* extract distinct datetimes from fact table and store it in a time dimension table\n",
    "* add other complementary fields as year, month, week, etc.\n",
    "* write time dimension table to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Dimension Demographics**\n",
    "* It's only US cities\n",
    "* no further cleaning needed\n",
    "* create demographics table\n",
    "* write demographics table to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Dimension Airports**  \n",
    "* It's world airports: keep only US airports\n",
    "* remove rowd with missing values in 'municipality' field\n",
    "* extract state from 'iso_region' field\n",
    "* extract latitude/longitude from 'coordinates' field\n",
    "* create airports dimension table\n",
    "* write airports dimension table to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Dimension Temperature**  \n",
    "* remove rows with missing 'AverageTemperature' (we could have imputed missing values with closest ones in terms of date/city or even lat/long, but in this instance I think could be risky as it would much deeper investigation to understand for example if there are big period gaps in terms of days for a specific City, which could result in a very imprecise assignation)\n",
    "- remove latitude/longitude fields\n",
    "- remove dates prior to 1960 (when [mass air travel began](https://www.insider.com/air-travel-in-every-decade-2017-8) ). The alternative is to keep only dates greater than min(date) in dim_time table, to make sure to cover all dates included in fact table (I commented this condition below, but not included for now as our fact table has only 2016 data).\n",
    "- compute avg. statistics by day/city/country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below we organised the code into several function and tested one by one (output tables are written locally instead of into S3). However you can find the **whole ETL pipeline** in the provided **etl.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"Create or retrieve a Spark session\n",
    "    \"\"\"\n",
    "    \n",
    "    spark = SparkSession.builder.\\\n",
    "        config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "        enableHiveSupport().\\\n",
    "        getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"Extract immigration data from provided folder in local machine, process it using Spark and \n",
    "    loads it into S3 as a set of tables in parquet format:\n",
    "        - a fact table with immigration data and \n",
    "        - a dimension table with correspondent dates available\n",
    "    @spark: spark session defined previously\n",
    "    @input_data: location path for the dataset\n",
    "    @output_data: S3 bucket name where to load processed output data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Read in immigration dataset\n",
    "    print('Loading immigration data...')\n",
    "    #df_spark = spark.read.csv('immigration_data_sample.csv', header = 'True') # to use sample dataset\n",
    "    df_spark = spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "        load(input_data)\n",
    "\n",
    "    \n",
    "    # Read in mapping codes datasets\n",
    "    print('Loading mapping codes...')\n",
    "    map_mode_codes = spark.read.csv('map_codes/map_mode_codes.csv', header = 'True')\n",
    "    map_visa_codes = spark.read.csv('map_codes/map_visa_codes.csv', header = 'True')\n",
    "    map_state_codes = spark.read.csv('map_codes/map_state_codes.csv', header = 'True')\n",
    "\n",
    "    map_port_codes = spark.read.csv('map_codes/map_port_codes.csv', header = 'True')\n",
    "    map_country_codes = spark.read.csv('map_codes/map_country_codes.csv', header = 'True')\n",
    "    \n",
    "    # Create temporary tables to be able to use SparkSQL\n",
    "    print('Processing immigration data...')\n",
    "    #df_spark_sample.createOrReplaceTempView(\"df_immigration\")  # to use sample dataset\n",
    "    df_spark.createOrReplaceTempView(\"df_immigration\")  # to use full dataset\n",
    "    map_mode_codes.createOrReplaceTempView(\"map_mode_codes\")\n",
    "    map_visa_codes.createOrReplaceTempView(\"map_visa_codes\")\n",
    "    map_state_codes.createOrReplaceTempView(\"map_state_codes\")\n",
    "\n",
    "    #map_port_codes.createOrReplaceTempView(\"map_port_codes\")\n",
    "    #map_country_codes.createOrReplaceTempView(\"map_country_codes\")\n",
    "    \n",
    "    # Join immigration data with mapping codes, convert SAS dates and rename fields\n",
    "    df_immigration_joined = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            i.i94yr as year,\n",
    "            i.i94mon as month,\n",
    "            i.i94cit as citizenship_country,\n",
    "            i.i94res as residence_country,\n",
    "            i.i94port as port,\n",
    "            i.arrdate,\n",
    "            date_add(to_date('1960-01-01'), i.arrdate) AS arrival_date,\n",
    "            coalesce(m.mode, 'Not reported') as arrival_mode,\n",
    "            coalesce(s.code, 'Not reported') as us_state,\n",
    "            i.depdate,\n",
    "            date_add(to_date('1960-01-01'), i.depdate) AS departure_date,\n",
    "            i.i94bir as respondent_age,\n",
    "            coalesce(v.visa, 'Not reported') as visa,\n",
    "            i.dtadfile as date_added,\n",
    "            i.visapost as visa_issued_department,\n",
    "            i.occup as occupation,\n",
    "            i.entdepa as arrival_flag,\n",
    "            i.entdepd as departure_flag,\n",
    "            i.entdepu as update_flag,\n",
    "            i.matflag as match_arrival_departure_fag,\n",
    "            i.biryear as birth_year,\n",
    "            i.dtaddto as allowed_to_stay_date,\n",
    "            i.insnum as ins_number,\n",
    "            i.airline as airline,\n",
    "            i.admnum as admission_number,\n",
    "            i.fltno as flight_number,\n",
    "            i.visatype as visa_type\n",
    "        from df_immigration i \n",
    "        left join map_mode_codes m on i.i94mode = m.code\n",
    "        left join map_visa_codes v on i.i94visa = v.code\n",
    "        left join map_state_codes s on i.i94addr = s.code\n",
    "    \"\"\")\n",
    "    \n",
    "    # Additional cleaning and prepare final fact table\n",
    "    df_immigration_joined.createOrReplaceTempView(\"df_immigration_joined\")\n",
    "    df_immigration_cleaned = spark.sql(\"\"\"\n",
    "        select *\n",
    "        from df_immigration_joined\n",
    "        where 1=1\n",
    "            and respondent_age >= 0\n",
    "            and arrival_date IS NOT NULL \n",
    "            and departure_date IS NOT NULL\n",
    "            and arrival_date <= departure_date\n",
    "    \"\"\")\n",
    "    \n",
    "    fact_immigration = df_immigration_cleaned.\\\n",
    "        drop('arrdate', 'depdate')\n",
    "    \n",
    "    # Write fact table into S3 in parquet format\n",
    "    print('Writing immigration fact table in parquet...')\n",
    "    \n",
    "    fact_immigration.write.partitionBy(\"year\", \"month\", \"us_state\").parquet((output_data+\"fact_immigration\"),'overwrite')\n",
    "    print('Fact immigration table created in parquet')\n",
    "    \n",
    "    # Generate a dimension time table using the Immigration fact table\n",
    "    print('Extracting time data...')\n",
    "    fact_immigration.createOrReplaceTempView(\"fact_immigration\")\n",
    "    dim_time = spark.sql(\"\"\"\n",
    "        SELECT DISTINCT arrival_date AS date\n",
    "        FROM fact_immigration\n",
    "        WHERE arrival_date IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT departure_date AS date\n",
    "        FROM fact_immigration\n",
    "        WHERE departure_date IS NOT NULL\n",
    "    \"\"\")\n",
    "\n",
    "    dim_time.createOrReplaceTempView(\"dim_time\")\n",
    "    \n",
    "    \n",
    "    dim_time = dim_time.select(\n",
    "        col('date').alias('date'),\n",
    "        dayofmonth('date').alias('day'),\n",
    "        weekofyear('date').alias('week'),\n",
    "        month('date').alias('month'),\n",
    "        year('date').alias('year')\n",
    "    )\n",
    "    \n",
    "    # Write dim time table into S3 in parquet format\n",
    "    print('Writing time dimension table in parquet...')\n",
    "    dim_time.write.parquet((output_data+\"dim_time\"),'overwrite')\n",
    "    print('Time dimension table created in parquet')\n",
    "    \n",
    "    # Write also mapping tables for port_codes and country_codes in parquet format: useful to join by city/country\n",
    "    print('Writing mapping tables in parquet...')\n",
    "    map_port_codes.write.parquet((output_data+\"map_port_codes\"),'overwrite')\n",
    "    map_country_codes.write.parquet((output_data+\"map_country_codes\"),'overwrite')\n",
    "    map_state_codes.write.parquet((output_data+\"map_state_codes\"),'overwrite')\n",
    "    print('Mapping tables created in parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading immigration data...\n",
      "Loading mapping codes...\n",
      "Processing immigration data...\n",
      "Writing immigration fact table in parquet...\n",
      "Fact immigration table created in parquet\n",
      "Extracting time data...\n",
      "Writing time dimension table in parquet...\n",
      "Time dimension table created in parquet\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "input_data = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "output_data = \"2016_04/\"\n",
    "process_immigration_data(spark=spark, input_data=input_data, output_data=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2953435\n",
      "+-------------------+-----------------+----+------------+------------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+------+-----+--------+\n",
      "|citizenship_country|residence_country|port|arrival_date|arrival_mode|departure_date|respondent_age|    visa|date_added|visa_issued_department|occupation|arrival_flag|departure_flag|update_flag|match_arrival_departure_fag|birth_year|allowed_to_stay_date|ins_number|airline|admission_number|flight_number|visa_type|  year|month|us_state|\n",
      "+-------------------+-----------------+----+------------+------------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+------+-----+--------+\n",
      "|              254.0|            209.0| AGA|  2016-04-30|         Air|    2016-05-02|          62.0|Pleasure|  20160430|                  null|      null|           G|             O|       null|                          M|    1954.0|            07282016|      null|     UA| 5.9504946533E10|        00150|       WT|2016.0|  4.0|      GU|\n",
      "|              254.0|            209.0| AGA|  2016-04-30|         Air|    2016-05-02|          44.0|Pleasure|  20160430|                  null|      null|           G|             O|       null|                          M|    1972.0|            07282016|      null|     UA| 5.9504949233E10|        00150|       WT|2016.0|  4.0|      GU|\n",
      "|              254.0|            209.0| AGA|  2016-04-30|         Air|    2016-05-03|          71.0|Pleasure|  20160430|                  null|      null|           G|             O|       null|                          M|    1945.0|            07292016|      null|     UA| 5.9513706133E10|        00178|       WT|2016.0|  4.0|      GU|\n",
      "+-------------------+-----------------+----+------------+------------+--------------+--------------+--------+----------+----------------------+----------+------------+--------------+-----------+---------------------------+----------+--------------------+----------+-------+----------------+-------------+---------+------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read fact table \n",
    "table=spark.read.parquet(\"2016_04/fact_immigration\")\n",
    "print(table.count())\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "+----------+---+----+-----+----+\n",
      "|      date|day|week|month|year|\n",
      "+----------+---+----+-----+----+\n",
      "|2016-06-30| 30|  26|    6|2016|\n",
      "|2016-06-03|  3|  22|    6|2016|\n",
      "|2016-07-20| 20|  29|    7|2016|\n",
      "+----------+---+----+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read dim time table \n",
    "table=spark.read.parquet(\"2016_04/dim_time\")\n",
    "print(table.count())\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_airports_data(spark, input_data, output_data):\n",
    "    \"\"\"Extract airport data from provided folder in local machine, process it using Spark and \n",
    "    loads it into S3 as a dimension table in parquet format:\n",
    "    @spark: spark session defined previously\n",
    "    @input_data: location path for the dataset\n",
    "    @output_data: S3 bucket name where to load processed output data\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read in airport data\n",
    "    print('Loading airport data...')\n",
    "    df_airports = spark.read.csv(input_data, header = 'True')\n",
    "    \n",
    "    #Cleaning: keep only US data, extract state and lat/long\n",
    "    print('Processing airports data...')\n",
    "    df_airports_us_clean = df_airports.filter(\"iso_country == 'US'\")\\\n",
    "        .withColumn(\"state\", split(col(\"iso_region\"), \"-\")[1])\\\n",
    "        .withColumn(\"latitude\", split(col(\"coordinates\"), \",\")[0].cast(Dbl()))\\\n",
    "        .withColumn(\"longitude\", split(col(\"coordinates\"), \",\")[1].cast(Dbl()))\n",
    "    \n",
    "    dim_airports = df_airports_us_clean.\\\n",
    "        drop('continent', 'iso_region', 'coordinates')\n",
    "    \n",
    "    # Write dim table into S3 in parquet format\n",
    "    print('Writing airports dimension table in parquet...')\n",
    "    \n",
    "    dim_airports.write.partitionBy(\"state\").parquet((output_data+\"dim_airports\"),'overwrite')\n",
    "    print('Dim airports table created in parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading airport data...\n",
      "Processing airports data...\n",
      "Writing airports dimension table in parquet...\n",
      "Dim airports table created in parquet\n"
     ]
    }
   ],
   "source": [
    "#spark = create_spark_session()\n",
    "input_data = 'airport-codes_csv.csv'\n",
    "output_data = \"2016_04/\"\n",
    "process_airports_data(spark=spark, input_data=input_data, output_data=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22757\n",
      "+-----+-------------+--------------------+------------+-----------+------------+--------+---------+----------+------------------+------------------+-----+\n",
      "|ident|         type|                name|elevation_ft|iso_country|municipality|gps_code|iata_code|local_code|          latitude|         longitude|state|\n",
      "+-----+-------------+--------------------+------------+-----------+------------+--------+---------+----------+------------------+------------------+-----+\n",
      "| 00TA|     heliport|Sw Region Faa Hel...|         598|         US|  Fort Worth|    00TA|     null|      00TA|-97.30580139160156|32.826900482177734|   TX|\n",
      "| 00TE|     heliport|Tcjc-Northeast Ca...|         600|         US|  Fort Worth|    00TE|     null|      00TE|-97.18949890136719|32.847599029541016|   TX|\n",
      "| 00TS|small_airport|Alpine Range Airport|         670|         US|     Everman|    00TS|     null|      00TS|-97.24199676513672|32.607601165771484|   TX|\n",
      "+-----+-------------+--------------------+------------+-----------+------------+--------+---------+----------+------------------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read dim airports table \n",
    "table=spark.read.parquet(\"2016_04/dim_airports\")\n",
    "print(table.count())\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_cities_data(spark, input_data, output_data):\n",
    "    \"\"\"Extract US cities demographics data from provided folder in local machine, process it using Spark and \n",
    "    loads it into S3 as a dimension table in parquet format:\n",
    "    @spark: spark session defined previously\n",
    "    @input_data: location path for the dataset\n",
    "    @output_data: S3 bucket name where to load processed output data\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read in US cities data\n",
    "    print('Loading cities data...')\n",
    "    df_demographics = spark.read.csv(input_data, header = 'True', sep = \";\")\n",
    "    #pd.read_csv(input_data, sep=';')\n",
    "    \n",
    "    #Cleaning: no cleaning needed\n",
    "    print('Processing cities data...')\n",
    "    df_demographics_clean = df_demographics.select(\n",
    "        col('City').alias('city'),\n",
    "        col('State').alias('state_name'),\n",
    "        col('Median Age').alias('median_age'),\n",
    "        col('Male Population').alias('male_population'),\n",
    "        col('Female Population').alias('female_population'),\n",
    "        col('Number of Veterans').alias('number_veterans'),\n",
    "        col('Foreign-born').alias('foreign_born'),\n",
    "        col('Average Household Size').alias('avg_household_size'),\n",
    "        col('State Code').alias('state_code'),\n",
    "        col('Race').alias('race'),\n",
    "        col('Count').alias('count'),\n",
    "    )\n",
    "\n",
    "    \n",
    "    dim_cities_demographics = df_demographics_clean\n",
    "    \n",
    "    # Write dim table into S3 in parquet format\n",
    "    print('Writing cities dimension table in parquet...')\n",
    "    \n",
    "    dim_cities_demographics.write.partitionBy(\"state_code\").parquet((output_data+\"dim_cities_demographics\"),'overwrite')\n",
    "    print('Dim cities table created in parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cities data...\n",
      "Processing cities data...\n",
      "Writing cities dimension table in parquet...\n",
      "Dim cities table created in parquet\n"
     ]
    }
   ],
   "source": [
    "#spark = create_spark_session()\n",
    "input_data = 'us-cities-demographics.csv'\n",
    "output_data = \"2016_04/\"\n",
    "process_cities_data(spark=spark, input_data=input_data, output_data=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2891\n",
      "+----------------+----------+----------+---------------+-----------------+---------------+------------+------------------+--------------------+-----+----------+\n",
      "|            city|state_name|median_age|male_population|female_population|number_veterans|foreign_born|avg_household_size|                race|count|state_code|\n",
      "+----------------+----------+----------+---------------+-----------------+---------------+------------+------------------+--------------------+-----+----------+\n",
      "|Rancho Cucamonga|California|      34.5|          88127|            87105|           5821|       33878|              3.18|Black or African-...|24437|        CA|\n",
      "|     West Covina|California|      39.8|          51629|            56860|           3800|       37038|              3.56|               Asian|32716|        CA|\n",
      "|          Folsom|California|      40.9|          41051|            35317|           4187|       13234|              2.62|  Hispanic or Latino| 5822|        CA|\n",
      "+----------------+----------+----------+---------------+-----------------+---------------+------------+------------------+--------------------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read dim cities table \n",
    "table=spark.read.parquet(\"2016_04/dim_cities_demographics\")\n",
    "print(table.count())\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"Extract world temperature data from provided folder in local machine, process it using Spark and \n",
    "    loads it into S3 as a dimension table in parquet format:\n",
    "    @spark: spark session defined previously\n",
    "    @input_data: location path for the dataset\n",
    "    @output_data: S3 bucket name where to load processed output data\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read in US cities data\n",
    "    print('Loading temperature data...')\n",
    "    df_temperature = spark.read.csv(input_data, header = 'True')\n",
    "    df_temperature.createOrReplaceTempView(\"df_temperature\")\n",
    "    \n",
    "    #Cleaning: remove missing data, unnecessary fields and dates and compute avg. statistics by date/city/country\n",
    "    print('Processing temperature data...')\n",
    "    df_temperature_clean = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            dt as date,\n",
    "            City as city,\n",
    "            Country as country,\n",
    "            avg(AverageTemperature) as avg_temperature,\n",
    "            avg(AverageTemperatureUncertainty) as avg_temperature_uncertainty\n",
    "            --count(*)\n",
    "        FROM df_temperature\n",
    "        WHERE 1=1\n",
    "            and dt >='1960-01-01'\n",
    "            --and dt >= (select min(date) from dim_time) --make sure we cover all dates we have in the fact table\n",
    "            and AverageTemperature is not null\n",
    "        group by date, city, country\n",
    "    \"\"\")\n",
    "\n",
    "    \n",
    "    dim_world_temperatures = df_temperature_clean\n",
    "    \n",
    "    # Write dim table into S3 in parquet format\n",
    "    print('Writing temperature dimension table in parquet...')\n",
    "    \n",
    "    dim_world_temperatures.write.partitionBy(\"country\").parquet((output_data+\"dim_world_temperatures\"),'overwrite')\n",
    "    print('Dim temperatures table created in parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading temperature data...\n",
      "Processing temperature data...\n",
      "Writing temperature dimension table in parquet...\n",
      "Dim temperatures table created in parquet\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "input_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "output_data = \"2016_04/\"\n",
    "process_temperature_data(spark=spark, input_data=input_data, output_data=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247991\n",
      "+----------+------+---------------+---------------------------+-------+\n",
      "|      date|  city|avg_temperature|avg_temperature_uncertainty|country|\n",
      "+----------+------+---------------+---------------------------+-------+\n",
      "|1979-04-01|Abohar|         28.851|        0.28300000000000003|  India|\n",
      "|1983-03-01|Abohar|         20.208|                      0.179|  India|\n",
      "|1991-06-01|Abohar|         35.069|                      0.469|  India|\n",
      "+----------+------+---------------+---------------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read dim temperatures table \n",
    "table=spark.read.parquet(\"2016_04/dim_world_temperatures\")\n",
    "print(table.count())\n",
    "table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "The following function perform quality checks against each the fact/dimension tables created:\n",
    "- count records to ensure completeness\n",
    "- display schema and data types\n",
    "- raise an error if the count of records is < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_quality_checks():\n",
    "    tables = ['fact_immigration', 'dim_time', 'dim_airports', 'dim_cities_demographics', 'dim_world_temperatures']\n",
    "    for i in tables:\n",
    "        print(i)\n",
    "        path = \"2016_04/\"+ i\n",
    "        #print(path)\n",
    "        count_records = spark.read.parquet(path).count()\n",
    "        #print(count_records)\n",
    "        if count_records < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {i} contained 0 rows\")\n",
    "        print(f\"Data quality check on table {i} passed with {count_records} records\") \n",
    "        print(\"Showing schema:\")\n",
    "        spark.read.parquet(path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_immigration\n",
      "Data quality check on table fact_immigration passed with 2953435 records\n",
      "Showing schema:\n",
      "root\n",
      " |-- citizenship_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- respondent_age: double (nullable = true)\n",
      " |-- visa: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- visa_issued_department: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- arrival_flag: string (nullable = true)\n",
      " |-- departure_flag: string (nullable = true)\n",
      " |-- update_flag: string (nullable = true)\n",
      " |-- match_arrival_departure_fag: string (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- allowed_to_stay_date: string (nullable = true)\n",
      " |-- ins_number: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admission_number: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- us_state: string (nullable = true)\n",
      "\n",
      "dim_time\n",
      "Data quality check on table dim_time passed with 175 records\n",
      "Showing schema:\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "dim_airports\n",
      "Data quality check on table dim_airports passed with 22757 records\n",
      "Showing schema:\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "dim_cities_demographics\n",
      "Data quality check on table dim_cities_demographics passed with 2891 records\n",
      "Showing schema:\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- median_age: string (nullable = true)\n",
      " |-- male_population: string (nullable = true)\n",
      " |-- female_population: string (nullable = true)\n",
      " |-- number_veterans: string (nullable = true)\n",
      " |-- foreign_born: string (nullable = true)\n",
      " |-- avg_household_size: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n",
      "dim_world_temperatures\n",
      "Data quality check on table dim_world_temperatures passed with 2247991 records\n",
      "Showing schema:\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- avg_temperature: double (nullable = true)\n",
      " |-- avg_temperature_uncertainty: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_quality_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- The dictionary is provided inside **data_dictionary.txt** file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Given the already big size of both immigration data (about 3M rows) and world temperature dataset (8M rows), we decided to go for a leading big data framework such as **Apache Spark**. Also, thinking about the potential future increase in the amount of data (new immigrants records and temperature registrations), a technology such Spark will enable us to build ETL pipelines taking advantage of all the distributed processing techniques with an efficient use of memory.\n",
    "- Moreover, being already familiar with SQL and Python's Pandas library, the SparkSQL module was a great incentive for choosing this technology. \n",
    "<br/>\n",
    "- In terms of storage we chose to store our final fact/dimension table in **Amazon S3** in Parquet format, which offer an easy & flexible solution for hosting a Data Lake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Other possible scenarios to consider:  \n",
    "- data increases by 100x: we would use a cluster like Amazon EMR and probably increase the number of nodes accordingly. We would still use Apache Spark for data processing and a service like Amazon S3 for storage.\n",
    "- data populates a dashboard that must be updated on a daily basis by 7am every day: we would create the data pipeline with a tool like Apache Airflow which will help us coordinating and scheduling all the data processing steps as well as monitoring data quality. The data that will require a daily scheduling will probably be the immigration and possibly the world temperatures. While for cities and airports the updates will be much less frequent.\n",
    "- database needed to be accessed by 100+ people: with current tables stored and partitioned as they are in S3 it should be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
